{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook summarises how yellow taxi data from 2019 and 2020 was cleaned. Steps and analysis can be found in **2_Preprocessing_Taxi.ipynb**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import time\n",
    "import gc\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/14 11:32:51 WARN Utils: Your hostname, LAPTOP-38BBQ5BL resolves to a loopback address: 127.0.1.1; using 172.18.108.55 instead (on interface eth0)\n",
      "21/08/14 11:32:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/zhaoling/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/14 11:32:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:SparkMonitorKernel:Client Connected ('127.0.0.1', 55178)\n"
     ]
    }
   ],
   "source": [
    "# Only run this cell below if you have installed SparkMonitor\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Start the spark context\n",
    "sc = SparkContext.getOrCreate(conf=swan_spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session (which will run spark jobs)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Directly use Apache Arrow with pip3 install pyarrow\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct schema\n",
    "schema = StructType()\n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField(\"VendorID\", StringType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"RatecodeID\", StringType(), True), \n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True), \n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", StringType(), True), \n",
    "    StructField(\"mta_tax\", StringType(), True), \n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", StringType(), True),\n",
    "    StructField(\"improvement_surcharge\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", StringType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sdf, year):\n",
    "    drop_cols = ['VendorID', 'RateCodeID', \n",
    "           'extra', 'mta_tax', 'tolls_amount', 'improvement_surcharge', \n",
    "           'congestion_surcharge', 'store_and_fwd_flag']\n",
    "    sdf.drop(*drop_cols)\n",
    "    if year == '2019':\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            *\n",
    "        FROM\n",
    "            sdf\n",
    "        WHERE \n",
    "            sdf.total_amount < 34\n",
    "            AND sdf.total_amount > 0\n",
    "            AND sdf.trip_distance < 10\n",
    "            AND sdf.trip_distance > 0\n",
    "            AND sdf.passenger_count > 0\n",
    "            AND sdf.tpep_pickup_datetime >= '2019-01-01'\n",
    "            AND sdf.tpep_pickup_datetime < '2020-01-01'\n",
    "            AND sdf.tpep_dropoff_datetime >= '2019-01-01'\n",
    "            AND sdf.tpep_dropoff_datetime < '2020-01-01'\n",
    "            AND sdf.PULocationID >= 1\n",
    "            AND sdf.PULocationID <= 263\n",
    "            AND sdf.DOLocationID >= 1\n",
    "            AND sdf.DOLocationID <= 263\n",
    "        \"\"\"\n",
    "    else:\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            *\n",
    "        FROM\n",
    "            sdf\n",
    "        WHERE \n",
    "            sdf.total_amount < 34\n",
    "            AND sdf.total_amount > 0\n",
    "            AND sdf.trip_distance < 10\n",
    "            AND sdf.trip_distance > 0\n",
    "            AND sdf.passenger_count > 0\n",
    "            AND sdf.tpep_pickup_datetime >= '2020-01-01'\n",
    "            AND sdf.tpep_pickup_datetime < '2021-01-01'\n",
    "            AND sdf.tpep_dropoff_datetime >= '2020-01-01'\n",
    "            AND sdf.tpep_dropoff_datetime < '2021-01-01'\n",
    "            AND sdf.PULocationID >= 1\n",
    "            AND sdf.PULocationID <= 263\n",
    "            AND sdf.DOLocationID >= 1\n",
    "            AND sdf.DOLocationID <= 263\n",
    "        \"\"\"\n",
    "    sdf.createOrReplaceTempView(\"sdf\")\n",
    "    result = spark.sql(query)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_2019 = spark.read.csv('../raw_data/2019', header=True, schema = customSchema)\n",
    "cleaned_19 = preprocessing(sdf_2019, '2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_2020 = spark.read.csv('../raw_data/2020', header=True, schema = customSchema)\n",
    "cleaned_20 = preprocessing(sdf_2020, '2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate by day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_daily(sdf):\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "            TO_DATE(tpep_pickup_datetime) AS pickup_date,\n",
    "            ROUND(COUNT(*)/1000, 2) AS trip_count_k,\n",
    "            MEAN(total_amount) AS Average_Trip_Amount_USD,\n",
    "            MEAN(trip_distance) AS Average_Distance_in_Miles,\n",
    "            MEAN(passenger_count) AS Average_passenger_count\n",
    "        FROM\n",
    "            sdf\n",
    "        GROUP BY\n",
    "            pickup_date\n",
    "    \"\"\"\n",
    "    sdf.createOrReplaceTempView(\"sdf\")\n",
    "    result = spark.sql(query)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_19 = aggregate_daily(cleaned_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------------------+-------------------------+-----------------------+\n",
      "|pickup_date|trip_count_k|Average_Trip_Amount_USD|Average_Distance_in_Miles|Average_passenger_count|\n",
      "+-----------+------------+-----------------------+-------------------------+-----------------------+\n",
      "| 2019-05-08|      247.37|       19.7860646717899|       3.0158937147292924|     1.5651650146338307|\n",
      "| 2019-06-04|      218.33|     18.957261484875914|       3.0430321531626063|     1.5280950854211515|\n",
      "| 2019-11-18|      217.47|     19.228767830013346|       2.9233298845818085|     1.5257322849128616|\n",
      "| 2019-09-22|      195.67|      19.73755049366429|        3.371832900627628|     1.6246090316249975|\n",
      "| 2019-11-01|      262.95|     19.229471154167534|        2.837496824491321|      1.580174938201179|\n",
      "| 2019-11-21|      248.96|     19.738632690074414|       2.9294314658011764|     1.5180915503141117|\n",
      "| 2019-05-27|      139.98|     19.920432130065947|       3.8215078474935873|      1.672626999378487|\n",
      "| 2019-02-23|      244.34|     17.471751669741842|       2.7581808434010138|     1.6766689585174999|\n",
      "| 2019-01-07|      217.44|     15.304864468345185|       2.8496785779985654|     1.5658342531272995|\n",
      "| 2019-01-08|      226.86|     15.255349683929156|         2.76634403293634|     1.5627253572656505|\n",
      "| 2019-01-28|      228.39|     15.492193207123439|        2.833124783809929|     1.5529080026446338|\n",
      "| 2019-10-05|      225.08|     18.228064271016812|        2.812368245674854|     1.6515252490247825|\n",
      "| 2019-03-17|      216.67|      18.98010232339449|        3.291189434429004|     1.6582204868322672|\n",
      "| 2019-04-28|      212.49|     18.713532323994443|        3.274980398985328|     1.6632108015003129|\n",
      "| 2019-01-30|      262.98|     15.006269421750895|        2.581124999049378|     1.5613701526363422|\n",
      "| 2019-03-14|      269.72|     19.414910646711615|       2.9707047783595133|      1.567226267629655|\n",
      "| 2019-07-30|      212.56|     18.966556939721215|       2.8542521100122884|      1.571858034042473|\n",
      "| 2019-05-14|      250.24|     19.133997602262063|       2.8358654891303625|      1.564418158567775|\n",
      "| 2019-07-08|       181.0|      20.10826295460851|        3.393478455444036|     1.5705729929225347|\n",
      "| 2019-07-28|      168.07|      19.25258584474194|        3.431993811918059|     1.6560437925802516|\n",
      "+-----------+------------+-----------------------+-------------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_20 = aggregate_daily(cleaned_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate by zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_pickup(sdf):\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "            PULocationID AS Pickup_location,\n",
    "            ROUND(COUNT(*)/1000, 2) AS trip_count_k\n",
    "        FROM\n",
    "            sdf\n",
    "        GROUP BY\n",
    "            PULocationID\n",
    "    \"\"\"\n",
    "    sdf.createOrReplaceTempView(\"sdf\")\n",
    "    result = spark.sql(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_dropoff(sdf):\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "            DOLocationID AS Dropoff_location,\n",
    "            ROUND(COUNT(*)/1000, 2) AS trip_count_k\n",
    "        FROM\n",
    "            sdf\n",
    "        GROUP BY\n",
    "            DOLocationID\n",
    "    \"\"\"\n",
    "    sdf.createOrReplaceTempView(\"sdf\")\n",
    "    result = spark.sql(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|Pickup_location|trip_count_k|\n",
      "+---------------+------------+\n",
      "|            148|     1004.57|\n",
      "|            243|       13.44|\n",
      "|             31|        0.42|\n",
      "|            137|      963.36|\n",
      "|             85|        3.25|\n",
      "|            251|        0.09|\n",
      "|             65|       93.99|\n",
      "|            255|       59.62|\n",
      "|             53|        1.07|\n",
      "|            133|        3.71|\n",
      "|             78|        2.48|\n",
      "|            155|        2.94|\n",
      "|            108|        1.02|\n",
      "|            211|      643.92|\n",
      "|             34|        1.83|\n",
      "|            193|       30.05|\n",
      "|            126|         1.6|\n",
      "|            101|        1.06|\n",
      "|            115|        0.11|\n",
      "|             81|         1.8|\n",
      "+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pickup_19 = aggregate_by_pickup(cleaned_19)\n",
    "pickup_19.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_19 = aggregate_by_dropoff(cleaned_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_20 = aggregate_by_pickup(cleaned_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_20 = aggregate_by_dropoff(cleaned_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Writing to Disk\n",
    "- Optional: csv, pickle, feather, or parquet\n",
    "\n",
    "Although it is more time-consuming to convert to pd and then csv, it will save time when reading the processed data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_date: date, trip_count_k: double, Average_Trip_Amount_USD: double, Average_Distance_in_Miles: double, Average_passenger_count: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_19.orderBy(col(\"pickup_date\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "daily_19.toPandas().to_csv('../processed_data/daily_summary_2019.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "daily_20_new = daily_20.sort(col(\"pickup_date\"))\n",
    "daily_20_new.toPandas().to_csv('../processed_data/daily_summary_2020.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pickup_19.toPandas().to_csv('../processed_data/pickup_location_tripcounts_2019.csv',header=True)\n",
    "dropoff_19.toPandas().to_csv('../processed_data/dropoff_location_tripcounts_2019.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pickup_20.toPandas().to_csv('../processed_data/pickup_location_tripcounts_2020.csv',header=True)\n",
    "dropoff_20.toPandas().to_csv('../processed_data/dropoff_location_tripcounts_2020.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
